{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d545b7d4-2869-4b82-9ffd-0d1efdabbbc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 200.50),\n",
    "    (2, \"Bob\", 150.75),\n",
    "    (3, \"Charlie\", 300.60),\n",
    "    (1, \"Alice\", 100.20),\n",
    "    (2, \"Bob\", 50.80),\n",
    "    (3, \"Charlie\", 120.30),\n",
    "    (4, \"David\", 400.00)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"purchase_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df1 = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e712878-9257-4e4b-8519-9992dcca86ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,col,sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708ea661-d106-47d5-868d-e1e4608b284d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # Aggregate total purchase amount per customer.\n",
    "agg_df = df1.groupBy(\"customer_id\", \"customer_name\").agg(sum(\"purchase_amount\").alias(\"total_amount\")).orderBy(col(\"total_amount\").desc())\n",
    "\n",
    "display(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141dd1be-2414-4588-85da-1eb75bfe0e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rank customers based on the highest total purchase.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "w = Window.orderBy(col('total_amount').desc())\n",
    "\n",
    "rank_df = agg_df.withColumn(\"rank\", rank().over(w)).select('customer_id','customer_name','total_amount')\n",
    "\n",
    "display(rank_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d663d25f-73b3-4910-9016-0397920fae44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch the top 3 customers.\n",
    "top3_df = rank_df.filter(col(\"rank\") <= 3)\n",
    "display(top3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fa7549-8c92-4c5c-8f0b-2484cf407c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"IT\", 70000),\n",
    "    (2, \"Bob\", \"HR\", 50000),\n",
    "    (3, \"Charlie\", \"IT\", 80000),\n",
    "    (4, \"David\", \"Finance\", 90000),\n",
    "    (5, \"Eve\", \"HR\", 55000),\n",
    "    (6, \"Frank\", \"IT\", 72000),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), True),\n",
    "    StructField(\"emp_name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data, schema)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb207a6-e06e-4d47-b9d5-80335830fcfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the average salary per department.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_sal_df = df2.groupBy(col('department')).agg(avg(col('salary')).alias('avg_salary'))\n",
    "\n",
    "display(avg_sal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119b5221-fd26-4c8f-bb99-1ddac9c55c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify employees earning above their departmentâ€™s average salary.\n",
    "\n",
    "final_df = df2.join(avg_sal_df, 'department','inner').filter(col('salary') > col('avg_salary')).select('emp_id','emp_name','department','salary','avg_salary')\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "844736bf-2a95-4050-bcdc-44581736ba01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [\n",
    "    (101, \"2024-01-10\", \"2024-01-12\"),\n",
    "    (102, \"2024-02-15\", \"2024-02-18\"),\n",
    "    (103, \"2024-03-01\", \"2024-03-03\"),\n",
    "    (104, \"2024-01-05\", \"2024-01-07\"),\n",
    "    (105, \"2024-02-20\", \"2024-02-25\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"delivery_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"order_date\", col(\"order_date\").cast(DateType()))\\\n",
    "       .withColumn(\"delivery_date\", col(\"delivery_date\").cast(DateType()))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35afcc1e-4836-4d6a-b4b2-d13640564daf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the delay for each order.\n",
    "delay_df = df.withColumn(\"delay\", col(\"delivery_date\") - col(\"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff05eccc-9aef-4706-a64c-ecfab3aa3783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Determine the average delay per month.\n",
    "from pyspark.sql.functions import month, avg\n",
    "\n",
    "delay_df.groupBy(month(col(\"order_date\"))).agg(avg(col(\"delay\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3817dcc5-ae41-4112-952d-1ff4c31c47e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType,StringType,StructField,StructType,IntegerType,DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"2024-03-20 10:15:00\", 150.00),\n",
    "    (1, \"Alice\", \"2024-03-20 11:45:00\", 200.00),\n",
    "    (1, \"Alice\", \"2024-03-20 14:30:00\", 50.00),\n",
    "    (1, \"Alice\", \"2024-03-20 17:10:00\", 300.00),  # Fraudulent (4 transactions)\n",
    "    (2, \"Bob\", \"2024-03-21 09:00:00\", 100.00),\n",
    "    (2, \"Bob\", \"2024-03-21 12:30:00\", 50.00),\n",
    "    (3, \"Charlie\", \"2024-03-22 08:15:00\", 75.00),\n",
    "    (3, \"Charlie\", \"2024-03-22 18:45:00\", 125.00),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"transaction_time\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "emp_df = spark.createDataFrame(data, schema)\\\n",
    "    .withColumn(\"transaction_time\", col(\"transaction_time\").cast(TimestampType()))\n",
    "\n",
    "emp_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cf27c9-8edf-4e59-9fcf-348d63f07175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, count\n",
    "\n",
    "final_df = emp_df.groupBy('customer_id','customer_name').agg(count('transaction_time').alias('total_transactions')).filter(col('total_transactions') > 3)\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb0c763-0c28-4a06-aa4d-227520834b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, StringType, IntegerType, StructType, StructField\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"2023-12-01\"),\n",
    "    (2, \"Bob\", \"2024-02-15\"),\n",
    "    (3, \"Charlie\", \"2023-11-20\"),\n",
    "    (4, \"David\", \"2024-01-10\"),\n",
    "    (5, \"Eve\", \"2023-09-05\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"last_purchase_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "emp1_df = spark.createDataFrame(data, schema)\\\n",
    "    .withColumn(\"last_purchase_date\", col(\"last_purchase_date\").cast(DateType()))\n",
    "\n",
    "emp1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a8de0eb-7d1f-4250-8ded-21b6c049c1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, months_between\n",
    "\n",
    "# Calculate months since last purchase\n",
    "df_churn = emp1_df.withColumn(\"months_since_purchase\", months_between(current_date(), \"last_purchase_date\"))\n",
    "\n",
    "# Identify customers who haven't made a purchase in the last 3 months\n",
    "df_churned_customers = df_churn.filter(df_churn.months_since_purchase > 3)\n",
    "\n",
    "df_churned_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe20bb1d-18e2-47f7-ad86-aa3eb733e994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "data = [\n",
    "    (101, \"Laptop\", \"2024-01-15\", 5),\n",
    "    (102, \"Phone\", \"2024-01-20\", 8),\n",
    "    (103, \"Tablet\", \"2024-01-25\", 3),\n",
    "    (104, \"Headphones\", \"2024-02-05\", 10),\n",
    "    (105, \"Monitor\", \"2024-02-10\", 6),\n",
    "    (106, \"Laptop\", \"2024-02-15\", 12),\n",
    "    (107, \"Phone\", \"2024-02-20\", 9),\n",
    "    (108, \"Tablet\", \"2024-03-01\", 4),\n",
    "    (109, \"Monitor\", \"2024-03-05\", 7),\n",
    "    (110, \"Phone\", \"2024-03-10\", 15),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True),\n",
    "    StructField(\"units_sold\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(data, schema)\\\n",
    "    .withColumn(\"sale_date\", col(\"sale_date\").cast(DateType()))\n",
    "\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08bf609-7aa5-4d12-83ca-18bcf5f437fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the top 3 most selling products each month using window functions.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, sum\n",
    "\n",
    "sales_df_agg = sales_df.groupBy('product_name', month('sale_date').alias('month')).agg(sum(col('units_sold')).alias('total_sold'))\n",
    "\n",
    "w = Window.partitionBy('month').orderBy(col('total_sold').desc())\n",
    "\n",
    "sales_final_df = sales_df_agg.withColumn('rank', rank().over(w)).filter(col('rank')<=3)\n",
    "\n",
    "display(sales_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a530ed8d-a705-4da7-a0ab-99e68939a82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# Extract year and month\n",
    "df_with_month = sales_df.withColumn(\"month\", month(\"sale_date\"))\n",
    "\n",
    "# Define a window partitioned by month and ordered by units_sold descending\n",
    "window_spec = Window.partitionBy(\"month\").orderBy(sales_df.units_sold.desc())\n",
    "\n",
    "# Rank products within each month\n",
    "df_ranked = df_with_month.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Get the top 3 products per month\n",
    "df_top_3_products = df_ranked.filter(df_ranked.rank <= 3)\n",
    "\n",
    "df_top_3_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e72cd0-dd32-41c1-adc8-90a228f3680e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"Alice\", \"2023-05-10\"),\n",
    "    (2, \"Bob\", \"2024-02-15\"),\n",
    "    (3, \"Charlie\", \"2023-07-20\"),\n",
    "    (4, \"David\", \"2023-12-25\"),\n",
    "    (5, \"Eve\", \"2024-01-05\"),\n",
    "    (6, \"Frank\", \"2023-08-30\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"last_purchase_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "emp2_df = spark.createDataFrame(data, schema)\\\n",
    "    .withColumn(\"last_purchase_date\", col(\"last_purchase_date\").cast(DateType()))\n",
    "\n",
    "emp2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df2f3a1-a897-40e6-ba6e-ad179f433f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify customers who made a purchase last year (2023) but have not made one in the current year (2024).\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "yearly_df = emp2_df.withColumn('year',year(col('last_purchase_date')))\n",
    "\n",
    "final_df = yearly_df.filter(col('year') == 2023).join(yearly_df.filter(col('year') == 2024), on='customer_id', how='leftanti')\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9aa6535-ea10-49f6-ad9f-18061a4fdedc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, \"Laptop\", 1200, 500, 50),   # High demand, low stock\n",
    "    (102, \"Phone\", 800, 200, 150),    # Normal demand\n",
    "    (103, \"Tablet\", 300, 80, 200),    # Low demand, high stock\n",
    "    (104, \"Headphones\", 150, 400, 50),  # High demand, low stock\n",
    "    (105, \"Monitor\", 400, 100, 120),  # Normal demand\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"demand\", IntegerType(), True),\n",
    "    StructField(\"inventory\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "sell_df = spark.createDataFrame(data, schema)\n",
    "sell_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572ae4b5-fee8-4452-822b-3e3ac01b19a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "final_df = sell_df.withColumn('adjusted_price',when((col('demand')>300) & (col('inventory')<100),col('price')*1.10)\\\n",
    "                                        .when((col('demand')<100) & (col('inventory')>150),col('price')*0.9)\\\n",
    "                                        .otherwise(col('price')))\n",
    "\n",
    "display(final_df)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b30cac09-c70d-457a-8799-7aa6fe954a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pricing = sell_df.withColumn(\"adjusted_price\",\n",
    "    when((sell_df.demand > 300) & (sell_df.inventory < 100), sell_df.price * 1.1)\n",
    "    .when((sell_df.demand < 100) & (sell_df.inventory > 150), sell_df.price * 0.9)\n",
    "    .otherwise(sell_df.price))\n",
    "\n",
    "df_pricing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d304a4-8b95-46c5-8b55-ba363cd25f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType, DoubleType, IntegerType, StringType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"2024-03-20 10:15:00\", 1500.00, \"New York\"),\n",
    "    (1, \"Alice\", \"2024-03-20 10:45:00\", 1600.00, \"Los Angeles\"),  # Suspicious\n",
    "    (2, \"Bob\", \"2024-03-21 12:30:00\", 500.00, \"Chicago\"),\n",
    "    (3, \"Charlie\", \"2024-03-22 14:00:00\", 3000.00, \"Miami\"),\n",
    "    (3, \"Charlie\", \"2024-03-22 14:50:00\", 3200.00, \"Houston\"),  # Suspicious\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"order_time\", StringType(), True),\n",
    "    StructField(\"order_amount\", DoubleType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "short_df = spark.createDataFrame(data, schema)\\\n",
    "    .withColumn(\"order_time\", col(\"order_time\").cast(TimestampType()))\n",
    "\n",
    "short_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4277cff8-9a79-4408-81e0-9312a86a8115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, expr, unix_timestamp\n",
    "\n",
    "w = Window.partitionBy('customer_id').orderBy(col('order_time'))\n",
    "\n",
    "new_df = short_df.withColumn('delay',lag(col('order_time')).over(w))\n",
    "\n",
    "new_df = new_df.withColumn('delay',unix_timestamp(col('order_time')) - unix_timestamp(col('delay')))\n",
    "\n",
    "df_suspicious = new_df.filter((col('delay')<3600) & (col('location') != lag(col('location')).over(w)))\n",
    "\n",
    "display(new_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Problems",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
