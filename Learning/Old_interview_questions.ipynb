{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef531e85-cdfc-4c93-a7e8-2a5ef7e46381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "word_df = spark.sparkContext.parallelize([\"akshaaya\"])\n",
    "\n",
    "# Count occurrences of 'a'\n",
    "a_count = word_df \\\n",
    "    .flatMap(lambda word: list(word)) \\\n",
    "    .filter(lambda char: char == 'a') \\\n",
    "    .count()\n",
    "\n",
    "print(f\"Number of 'a' characters: {a_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d36d68b-44d8-4f44-b353-4180ba971d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "367911b1-5da1-4482-9f64-a98da160f454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_data = [(1, '10#23#45')]\n",
    "input_schema = ['id', 'value']\n",
    "\n",
    "input_df = spark.createDataFrame(input_data, input_schema)\n",
    "\n",
    "input_df.withColumn('value', explode(split('value', '#'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "912777dc-9251-4776-b46e-e7e0e84edc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_record = [\n",
    "    ('hdfs', '2025-03-06 12:00:00', 'up'),\n",
    "    ('hdfs', '2025-03-06 12:01:00', 'up'),\n",
    "    ('hdfs', '2025-03-06 12:02:00', 'down'),\n",
    "    ('hdfs', '2025-03-06 12:03:00', 'down'),\n",
    "    ('hdfs', '2025-03-06 12:04:00', 'down'),\n",
    "    ('hdfs', '2025-03-06 12:05:00', 'down'),\n",
    "    ('hdfs', '2025-03-06 12:06:00', 'down'),\n",
    "    ('hdfs', '2025-03-06 12:07:00', 'up'),\n",
    "    ('hdfs', '2025-03-06 12:08:00', 'up'),\n",
    "    ('hdfs', '2025-03-06 12:09:00', 'down'),\n",
    "    ('hdfs', '2025-03-06 12:10:00', 'down')\n",
    "]\n",
    "\n",
    "test_schema = ['host', 'updated_time', 'status']\n",
    "\n",
    "test_df = spark.createDataFrame(test_record, test_schema)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192cc72e-e698-4786-87ce-6208a65b9e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, lead, col\n",
    "\n",
    "window = Window.partitionBy('host').orderBy('updated_time')\n",
    "\n",
    "lvl1 = test_df.withColumn('prev_status', lag('status').over(window)).withColumn('next_status', lead('status').over(window))\n",
    "\n",
    "lvl1.filter(((col('prev_status') == 'up') | (col('prev_status').isNull())) | ((col('next_status') == 'up') | (col('next_status').isNull())) & (col('status') == 'down')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef1e185-e383-4a0e-b3e1-b0fb022dcd22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, col, when, count, first, last, row_number, sum\n",
    "\n",
    "# First, let's create a window spec ordered by time\n",
    "w = Window.partitionBy('host').orderBy('updated_time')\n",
    "\n",
    "# Create a column to identify status changes\n",
    "df_with_changes = test_df.withColumn(\n",
    "    'status_changed',\n",
    "    when(\n",
    "        lag('status').over(w) != col('status'),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Create groups of consecutive statuses\n",
    "w_cumsum = Window.partitionBy('host').orderBy('updated_time')\n",
    "df_with_groups = df_with_changes.withColumn(\n",
    "    'group_id',\n",
    "    sum('status_changed').over(w_cumsum)\n",
    ")\n",
    "\n",
    "# Find sequences of 'down' status and their durations\n",
    "down_sequences = df_with_groups.filter(col('status') == 'down') \\\n",
    "    .groupBy('host', 'group_id') \\\n",
    "    .agg(\n",
    "        first('updated_time').alias('start_time'),\n",
    "        last('updated_time').alias('end_time'),\n",
    "        count('*').alias('duration')\n",
    "    )\n",
    "\n",
    "# Get the sequence with maximum duration\n",
    "max_downtime = down_sequences.orderBy(col('duration').desc()).limit(1) \\\n",
    "    .select('host', 'start_time', 'end_time')\n",
    "\n",
    "max_downtime.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31e1444-cac1-4c36-8bf7-bb6c2f79e870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, count, min, max\n",
    "\n",
    "# Create a window partitioned by host and ordered by time\n",
    "w = Window.partitionBy('host').orderBy('updated_time')\n",
    "\n",
    "# Get longest continuous down period in one step\n",
    "max_downtime = test_df.filter(col('status') == 'down') \\\n",
    "    .withColumn('group_time', \n",
    "                col('updated_time') - row_number().over(w)) \\\n",
    "    .groupBy('host', 'group_time') \\\n",
    "    .agg(\n",
    "        min('updated_time').alias('start_time'),\n",
    "        max('updated_time').alias('end_time'),\n",
    "        count('*').alias('duration')\n",
    "    ) \\\n",
    "    .orderBy(col('duration').desc()) \\\n",
    "    .limit(1) \\\n",
    "    .select('host', 'start_time', 'end_time')\n",
    "\n",
    "max_downtime.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4999ce3-72b6-4734-9aea-90abede3e6fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, col, when, count, first, last, sum\n",
    "\n",
    "# Single window spec since both use the same partitioning/ordering\n",
    "w = Window.partitionBy('host').orderBy('updated_time')\n",
    "\n",
    "max_downtime = test_df \\\n",
    "    .withColumn('group_id', \n",
    "                sum(when(lag('status').over(w) != col('status'), 1).otherwise(0))\n",
    "                .over(w)) \\\n",
    "    .filter(col('status') == 'down') \\\n",
    "    .groupBy('host', 'group_id') \\\n",
    "    .agg(\n",
    "        first('updated_time').alias('start_time'),\n",
    "        last('updated_time').alias('end_time')\n",
    "    ) \\\n",
    "    .orderBy(count('*').over(Window.partitionBy('host', 'group_id')).desc()) \\\n",
    "    .limit(1)\n",
    "\n",
    "max_downtime.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4995d397-984e-4ad7-8b21-c255c9a1a451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_file_name = '/FileStore/tables/file5.json'\n",
    "\n",
    "file_name = input_file_name.split('/')[-1]\n",
    "\n",
    "print(file_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Old_interview_questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
